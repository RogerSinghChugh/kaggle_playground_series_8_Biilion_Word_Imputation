{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3927,"databundleVersionId":44348,"sourceType":"competition"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# testing documentation code\n# from transformers import AutoTokenizer, AutoModelForMaskedLM\n\n# model_id = \"google-bert/bert-base-cased\"\n# tokenizer = AutoTokenizer.from_pretrained(model_id)\n# model = AutoModelForMaskedLM.from_pretrained(model_id)\n\n# text = \"The capital of France is [MASK].\"\n# inputs = tokenizer(text, return_tensors=\"pt\")\n# outputs = model(**inputs)\n\n# # To get predictions for the mask:\n# masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n# predicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\n# predicted_token = tokenizer.decode(predicted_token_id)\n# print(\"Predicted token:\", predicted_token)\n# # Predicted token:  Paris","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:11:06.625600Z","iopub.execute_input":"2026-02-14T08:11:06.625948Z","iopub.status.idle":"2026-02-14T08:11:07.142764Z","shell.execute_reply.started":"2026-02-14T08:11:06.625914Z","shell.execute_reply":"2026-02-14T08:11:07.141951Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Predicted token: Paris\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# unzip file\nimport zipfile\nimport os\n\nzip_path = \"/kaggle/input/competitions/billion-word-imputation/test_v2.txt.zip\"\nextract_to = \"/kaggle/working/\"\n\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(extract_to)\n\nprint(f\"Extracted to: {extract_to}\")\nprint(os.listdir(extract_to))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:15:01.584785Z","iopub.execute_input":"2026-02-14T08:15:01.585149Z","iopub.status.idle":"2026-02-14T08:15:02.163011Z","shell.execute_reply.started":"2026-02-14T08:15:01.585085Z","shell.execute_reply":"2026-02-14T08:15:02.162069Z"}},"outputs":[{"name":"stdout","text":"Extracted to: /kaggle/working/\n['.virtual_documents', 'test_v2.txt']\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nimport csv\nimport zipfile\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:28:21.430765Z","iopub.execute_input":"2026-02-14T08:28:21.431729Z","iopub.status.idle":"2026-02-14T08:28:21.437302Z","shell.execute_reply.started":"2026-02-14T08:28:21.431674Z","shell.execute_reply":"2026-02-14T08:28:21.436209Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# --- CONFIGURATION ---\nMODEL_NAME = \"google-bert/bert-base-cased\" \nINPUT_FILE = \"/kaggle/working/test_v2.txt\"\nOUTPUT_FILE = \"submission.csv\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Running on: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:28:23.799535Z","iopub.execute_input":"2026-02-14T08:28:23.799869Z","iopub.status.idle":"2026-02-14T08:28:23.805659Z","shell.execute_reply.started":"2026-02-14T08:28:23.799842Z","shell.execute_reply":"2026-02-14T08:28:23.804555Z"}},"outputs":[{"name":"stdout","text":"Running on: cuda\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\nmodel.to(device)\n# model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:28:54.705763Z","iopub.execute_input":"2026-02-14T08:28:54.706145Z","iopub.status.idle":"2026-02-14T08:28:55.228614Z","shell.execute_reply.started":"2026-02-14T08:28:54.706106Z","shell.execute_reply":"2026-02-14T08:28:55.227659Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"BertForMaskedLM(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (cls): BertOnlyMLMHead(\n    (predictions): BertLMPredictionHead(\n      (transform): BertPredictionHeadTransform(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (transform_act_fn): GELUActivation()\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"def solve_sentence(sentence, model, tokenizer, device):\n    \"\"\"\n    Finds the missing word by testing [MASK] at every possible position.\n    Returns: The reconstructed sentence with the missing word filled in.\n    \"\"\"\n    # Split by whitespace. This preserves the dataset's specific tokenization (e.g. \"word ,\")\n    words = sentence.strip().split()\n    \n    if len(words) == 0:\n        return sentence\n\n    # Generate Hypotheses: Insert [MASK] at every possible index (0 to len)\n    candidates_text = []\n    for i in range(len(words) + 1):\n        # Construct: words_before + [MASK] + words_after\n        candidate_words = words[:i] + [tokenizer.mask_token] + words[i:]\n        candidates_text.append(\" \".join(candidate_words))\n\n    # Tokenize all hypotheses as a single batch\n    # (BERT max length is 512, truncation prevents crashes on rare long sentences)\n    encoded_inputs = tokenizer(\n        candidates_text, \n        return_tensors=\"pt\", \n        padding=True, \n        truncation=True, \n        max_length=512\n    ).to(device)\n\n    # Inference (No Grad for speed/memory)\n    with torch.no_grad():\n        outputs = model(**encoded_inputs)\n        logits = outputs.logits\n\n    # Scoring Variables\n    best_score = -float('inf')\n    best_word = \"\"\n    best_insert_idx = -1\n\n    # Evaluate each hypothesis\n    # We look at the confidence of the model prediction at the [MASK] position\n    mask_token_id = tokenizer.mask_token_id\n    \n    for i in range(len(candidates_text)):\n        input_ids = encoded_inputs[\"input_ids\"][i]\n        \n        # Locate the mask in the tokenized sequence\n        # Note: Tokenizer might split words, so mask index != word index\n        mask_positions = (input_ids == mask_token_id).nonzero(as_tuple=True)[0]\n        \n        if len(mask_positions) == 0:\n            continue # Truncation might have cut it off\n            \n        mask_idx = mask_positions[0].item()\n\n        # Get logits for the mask\n        mask_logits = logits[i, mask_idx, :]\n        \n        # Get top prediction\n        probs = torch.softmax(mask_logits, dim=0)\n        top_prob, top_id = torch.topk(probs, 1)\n        \n        score = top_prob.item()\n        \n        # If this hypothesis is more confident than previous ones, pick it\n        if score > best_score:\n            best_score = score\n            best_word = tokenizer.decode([top_id.item()]).strip()\n            best_insert_idx = i\n\n    # Reconstruct the sentence using the original list to preserve spacing\n    final_words = words[:best_insert_idx] + [best_word] + words[best_insert_idx:]\n    return \" \".join(final_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:31:14.534858Z","iopub.execute_input":"2026-02-14T08:31:14.535234Z","iopub.status.idle":"2026-02-14T08:31:14.545734Z","shell.execute_reply.started":"2026-02-14T08:31:14.535202Z","shell.execute_reply":"2026-02-14T08:31:14.544612Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"df = pd.read_csv(INPUT_FILE, quotechar='\"')\nprint(f\"Total rows to process: {len(df)}\")\n\nresults = []\nids = []\n\nfor idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Imputing\"):\n    original_sent = row['sentence']\n    try:\n        fixed_sent = solve_sentence(original_sent, model, tokenizer, device)\n        results.append(fixed_sent)\n        ids.append(row['id'])        \n    except Exception as e:\n        print(f\"Error on ID {row['id']}: {e}\")\n        results.append(original_sent) # Fallback: return original\n        ids.append(row['id'])\n\n\nprint(f\"Saving to {OUTPUT_FILE}\")\nsubmission = pd.DataFrame({\n    \"id\": ids,\n    \"sentence\": results\n})\n\n# Kaggle Format: id,\"sentence\"\n# We use quoting=csv.QUOTE_NONNUMERIC to quote strings but not numbers\nsubmission.to_csv(\n    OUTPUT_FILE,\n    index=False,\n    sep=',',\n    encoding='utf-8',\n    quotechar='\"',\n    quoting=csv.QUOTE_NONNUMERIC,\n    doublequote=True\n)\n\nprint(\"Saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:31:17.314048Z","iopub.execute_input":"2026-02-14T08:31:17.314430Z","iopub.status.idle":"2026-02-14T08:32:58.805134Z","shell.execute_reply.started":"2026-02-14T08:31:17.314396Z","shell.execute_reply":"2026-02-14T08:32:58.804046Z"}},"outputs":[{"name":"stdout","text":"Total rows to process: 306681\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Imputing:   0%|          | 0/306681 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d80967cf7c4a41b7436e4f18785595"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1901416877.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moriginal_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mfixed_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolve_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3563745585.py\u001b[0m in \u001b[0;36msolve_sentence\u001b[0;34m(sentence, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Locate the mask in the tokenized sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Note: Tokenizer might split words, so mask index != word index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mmask_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmask_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_positions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":47},{"cell_type":"code","source":"print(\"\\n--- SANITY CHECK (First 5 Rows) ---\")\nwith open(OUTPUT_FILE, 'r') as f:\n    for i in range(5):\n        print(f.readline().strip())\nprint(\"-----------------------------------\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}